---
title: "30538 Problem Set 5: Web Scraping"
author: "Surya Hardiansyah and Astari Raihanah"
date: "today"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: true
  eval: true
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Surya Hardiansyah | sur
    - Partner 2 (name and cnet ID): Astari Raihanah | astari
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: **SH** **AR**
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: **00** Late coins left after submission: **04**
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings("ignore")
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup

# Prepare for Scraping
url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)

soup = BeautifulSoup(response.text, "lxml")

# Find all relevant elements within the specified container
containers = soup.find_all("div", class_="usa-card__container")

# Extract data from within the specified containers
titles = [container.find("h2", class_="usa-card__heading").get_text(strip=True) for container in containers]
dates = [container.find("span", class_="text-base-dark padding-right-105").get_text(strip=True) for container in containers]
categories = [container.find("li", class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1").get_text(strip=True) for container in containers]

# Extract links within the specified containers
link_content = [
    "https://oig.hhs.gov" + a["href"]
    for container in containers
    for a in container.find_all("a", href=True)
    if "/fraud/enforcement/" in a["href"]]

# Combine extracted data into a DataFrame
data = {
    "Title": titles,
    "Date": dates,
    "Category": categories,
    "Link": link_content[:len(titles)]  # Ensure the number of links matches the titles
}
df = pd.DataFrame(data)

# Print the head of df
print(df.head())
```


### 2. Crawling (PARTNER 1)

```{python}
# Function to extract agency information from a given link
def extract_agency_name(link):
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'lxml')
    
    # Find the div containing the action details
    details_div = soup.find('div', class_="margin-top-5 padding-y-3 border-y-1px border-base-lighter")
    if details_div:
        # Find all <li> elements within the div
        li_elements = details_div.find_all('li')
        for li in li_elements:
            # Check if the <li> contains 'Agency:' and extract the text after it
            if 'Agency:' in li.get_text():
                return li.get_text().replace('Agency:', '').strip()
    
    return 'N/A'

# Append agency info into df
df["Agency"] = df["Link"].apply(extract_agency_name)

# Print the head of df
print(df.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
```
DEFINE FUNCTION scrape_enforcement_actions(start_month, start_year)

1. Check if start year is valid:
IF start_year is before 2013:
Print a message: "Please restrict the start year to >= 2013"
RETURN (exit the function early)

2. Initialize variables:
SET results to an empty list (this will hold our scraped data)
SET start_date to the first day of start_month in start_year
SET current_date to today's date

3. Begin scraping from page 1:
SET page to 1 (we'll start at the first page)

4. Start looping through pages:
WHILE True (this will keep going until we break out):
- SET url to the enforcement page for the current page
- Print: "Scraping {url}..."
- GET the page content and parse it with BeautifulSoup

5. Find all enforcement action entries on the page:
- SET containers to all div elements with a specific class (these hold each enforcement action)

6. If no containers are found, stop:
- IF containers is empty:
    - BREAK the loop (we're done scraping)

7. Process each container found on the page:
- FOR EACH container in containers:
    - Extract:
        - title from the heading
        - date from a specific span tag
        - category from a list tag
        - link from the anchor tag's URL
    - IF date is missing, skip to the next container

8. Convert and check date:
- Convert date to a datetime object (date_obj)
- IF date_obj is earlier than start_date:
        - Print: "Encountered data older than the specified start date. Stopping scrape."
        - BREAK the loop (no need to look further)

9. If date is within range, save data:
- IF start_date <= date_obj <= current_date:
        - Add title, date, category, and link to results

10. Go to the next page:
- Increment page by 1
- WAIT for a second to avoid overloading the site

11. Retrieve agency names:
- FOR EACH link in results:
    - Call a separate function to retrieve the agency name from the link
    - Append the agency name to the corresponding entry in results

12. Save results as CSV:
- Convert results to a DataFrame
- Save as a CSV file named with the given start_year and start_month
- Print: "Data saved to [filename]"
```

In this way, we use a `while` loop to iterate through pages until we reach a stopping condition (either no more relevant data or data older than the start date). This structure allows us to handle the pages dynamically, stopping once weâ€™ve collected all the needed data without requiring a predefined number of iterations.


* b. Create Dynamic Scraper (PARTNER 2)
```{python}
import time
from datetime import datetime

# Function to extract agency information from a given link
def extract_agency_name(link):
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'lxml')
    
    # Find the div containing the action details
    details_div = soup.find('div', class_="margin-top-5 padding-y-3 border-y-1px border-base-lighter")
    if details_div:
        # Find all <li> elements within the div
        li_elements = details_div.find_all('li')
        for li in li_elements:
            # Check if the <li> contains 'Agency:' and extract the text after it
            if 'Agency:' in li.get_text():
                return li.get_text().replace('Agency:', '').strip()
    
    return 'N/A'

# Function to scrape enforcement actions starting from a specific month/year
def scrape_enforcement_actions(start_month, start_year):
    # Ensure start date is within range
    if start_year < 2013:
        print("Please restrict the start year to >= 2013")
        return

    # Initialize variables
    results = []
    start_date = datetime(start_year, start_month, 1)
    current_date = datetime.now()

    # Start from page 1 and continue ascendingly
    page = 1
    while True:
        url = f"https://oig.hhs.gov/fraud/enforcement/?page={page}"
        print(f"Scraping {url}...")
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'lxml')

        # Find all enforcement actions containers
        containers = soup.find_all('div', class_="usa-card__container")

        # Stop if there are no containers
        if not containers:
            break

        # Extract data from each container on the page
        for container in containers:
            title_elem = container.find('h2', class_="usa-card__heading")
            date_elem = container.find('span', class_="text-base-dark padding-right-105")
            category_elem = container.find('li', class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")
            link_elem = container.find('a', href=True)

            # Extract and validate each element
            title = title_elem.get_text(strip=True) if title_elem else "N/A"
            date = date_elem.get_text(strip=True) if date_elem else None
            category = category_elem.get_text(strip=True) if category_elem else "N/A"
            link = "https://oig.hhs.gov" + link_elem['href'] if link_elem else "N/A"

            # Only proceed if the date is present
            if date:
                # Convert date string to datetime object
                date_obj = datetime.strptime(date, "%B %d, %Y")

                # Stop scraping if the date is older than the start date
                if date_obj < start_date:
                    print("Encountered data older than the specified start date. Stopping scrape.")
                    break

                # Append data if it's within the range
                if start_date <= date_obj <= current_date:
                    results.append({
                        'Title': title,
                        'Date': date,
                        'Category': category,
                        'Link': link,
                        'Agency': None
                    })

        # Check if we stopped due to an older date and should break the loop
        if containers and date and date_obj < start_date:
            break

        # Move to the next page and wait before the next request
        page += 1
        time.sleep(1)

    # Second phase: Retrieve agency names
    for entry in results:
        entry['Agency'] = extract_agency_name(entry['Link'])

    # Convert results to DataFrame
    df = pd.DataFrame(results)

    # Save the DataFrame as CSV
    output_filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    df.to_csv(output_filename, index=False)
    print(f"Data saved to {output_filename}")

    return df

# Run the function to scrape from January 2023
autoscrape_df = scrape_enforcement_actions(1, 2023)

# Print results and check the earliest enforcement action
print(f"Total enforcement actions scraped: {len(autoscrape_df)}")
print("Earliest enforcement action scraped:")
print(autoscrape_df.sort_values(by='Date').head(1))
```

In this function, we first gather all titles, dates, categories, and links for each enforcement action without immediately fetching agency names. This approach is more efficient because it minimizes the number of sequential HTTP requests. After creating the initial database, we loop through each article's link to retrieve the agency name separately.


* c. Test Partner"s Code (PARTNER 1)

```{python}
# Run the function to scrape from January 2021
autoscrape_df = scrape_enforcement_actions(1, 2021)

# Print results and check the earliest enforcement action
print(f"Total enforcement actions scraped: {len(autoscrape_df)}")
print("Earliest enforcement action scraped:")
print(autoscrape_df.sort_values(by='Date').head(1))
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
# Convert Date column to datetime format
autoscrape_df["Date"] = pd.to_datetime(autoscrape_df["Date"], format="%B %d, %Y")

# Extract year and month for grouping
autoscrape_df["YearMonth"] = autoscrape_df["Date"].dt.to_period("M")

# Aggregate by YearMonth
monthly_count = autoscrape_df.groupby("YearMonth").size().reset_index(name="Count")

# Convert YearMonth back to datetime for plotting
monthly_count["YearMonth"] = monthly_count["YearMonth"].dt.to_timestamp()

# Create the line chart with dots
line_chart = alt.Chart(monthly_count).mark_line().encode(
    alt.X(
        'YearMonth:T',
        title="Month-Year",
        axis=alt.Axis(
            format='%b %Y',
            tickCount='year', 
            grid=True 
            )
    ),
    alt.Y('Count', title='Count')
).properties(
    title="Monthly Count of Enforcement Actions"
)

# Add points to indicate each data point on the line
points = alt.Chart(monthly_count).mark_point(size=5, color='blue').encode(
    x='YearMonth:T',
    y='Count:Q'
)

# Combine line and points
chart_with_dots = line_chart + points
chart_with_dots
```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```